{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import sys \n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPTextModel\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\Anaconda\\envs\\ldm\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/clip-vit-large-patch14\"  \n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for block in text_encoder.text_model.encoder.layers:\n",
    "    block.self_attn.q_proj.requires_grad_(True)\n",
    "    block.self_attn.k_proj.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [' rides on a bicycle',\n",
    " ' rides on a bike',\n",
    " ' rides on a motorcycle',\n",
    " ' rides on a horse']\n",
    "\n",
    "refer_subject = 'a man'\n",
    "infer_subject = 'a cat'\n",
    "\n",
    "refer_prompt = [refer_subject + prompt for prompt in prompts]\n",
    "infer_prompt = [infer_subject + prompt for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(prompt, token_indice):\n",
    "    inputs = processor(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    embeds = text_encoder(\n",
    "        inputs.input_ids, output_attentions=True\n",
    "    )\n",
    "\n",
    "    attn = embeds.attentions\n",
    "    last_layer_attn = torch.stack(attn)[-1].mean(dim=1)[:, token_indice, :token_indice+1]\n",
    "    \n",
    "    return last_layer_attn\n",
    "\n",
    "def compute_loss(value1, value2):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    return loss_fn(value1, value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_attn = get_prediction(refer_prompt, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0034488101955503225\n",
      "Epoch 2/5, Loss: 0.0034126455429941416\n",
      "Epoch 3/5, Loss: 0.003376640845090151\n",
      "Epoch 4/5, Loss: 0.003340776078402996\n",
      "Epoch 5/5, Loss: 0.003305052174255252\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(text_encoder.text_model.embeddings.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 5  # 设置更新次数\n",
    "for epoch in range(num_epochs):\n",
    "    # 获取两个 prompt 的 attention values    \n",
    "    pre_attn = get_prediction(infer_prompt, 8)\n",
    "\n",
    "    # 计算损失\n",
    "    loss = compute_loss(pre_attn, gt_attn)\n",
    "\n",
    "    # 反向传播并更新嵌入层\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    # 打印每次更新后的损失\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    infer_prompt,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "embeds = text_encoder(\n",
    "    inputs.input_ids, output_attentions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeds.last_hidden_state, 'new_embeds_all.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6926, 0.0215, 0.0319, 0.0274, 0.0333, 0.0320, 0.0206, 0.0767, 0.0640]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(embeds.attentions).mean(dim=0).mean(dim=1)[:,8,:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6700, 0.0226, 0.0474, 0.0256, 0.0325, 0.0308, 0.0217, 0.0872, 0.0623]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(embeds.attentions).mean(dim=0).mean(dim=1)[:,8,:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3884,  0.0229, -0.0522, -0.1841, -0.0273, -0.3355, -0.0176],\n",
       "         [ 0.0290, -1.3258,  0.3085, -0.0615,  0.0398, -0.7107, -0.9693],\n",
       "         [ 0.1177,  0.9112,  0.6360, -0.3315,  0.8024,  0.2287, -0.9680],\n",
       "         [-1.3853, -0.8009,  0.0932,  0.7572, -1.0690, -0.0540, -1.2084],\n",
       "         [-1.7795, -0.1171,  0.4707,  1.1404, -0.4600, -0.3640, -1.0923],\n",
       "         [-1.8796,  0.2238, -0.0867,  0.4162,  0.3026,  0.5182, -1.9219]],\n",
       "\n",
       "        [[-0.3884,  0.0229, -0.0522, -0.1841, -0.0273, -0.3355, -0.0176],\n",
       "         [ 0.0290, -1.3258,  0.3085, -0.0615,  0.0398, -0.7107, -0.9693],\n",
       "         [ 0.1177,  0.9112,  0.6360, -0.3315,  0.8024,  0.2287, -0.9680],\n",
       "         [-1.3853, -0.8009,  0.0932,  0.7572, -1.0690, -0.0540, -1.2084],\n",
       "         [-1.7795, -0.1171,  0.4707,  1.1404, -0.4600, -0.3640, -1.0923],\n",
       "         [-1.8796,  0.2238, -0.0867,  0.4162,  0.3026,  0.5182, -1.9219]],\n",
       "\n",
       "        [[-0.3884,  0.0229, -0.0522, -0.1841, -0.0273, -0.3355, -0.0176],\n",
       "         [ 0.0290, -1.3258,  0.3085, -0.0615,  0.0398, -0.7107, -0.9693],\n",
       "         [ 0.1177,  0.9112,  0.6360, -0.3315,  0.8024,  0.2287, -0.9680],\n",
       "         [-1.3853, -0.8009,  0.0932,  0.7572, -1.0690, -0.0540, -1.2084],\n",
       "         [-1.7795, -0.1171,  0.4707,  1.1404, -0.4600, -0.3640, -1.0923],\n",
       "         [-1.8796,  0.2238, -0.0867,  0.4162,  0.3026,  0.5182, -1.9219]],\n",
       "\n",
       "        [[-0.3884,  0.0229, -0.0522, -0.1841, -0.0273, -0.3355, -0.0176],\n",
       "         [ 0.0290, -1.3258,  0.3085, -0.0615,  0.0398, -0.7107, -0.9693],\n",
       "         [ 0.1177,  0.9112,  0.6360, -0.3315,  0.8024,  0.2287, -0.9680],\n",
       "         [-1.3853, -0.8009,  0.0932,  0.7572, -1.0690, -0.0540, -1.2084],\n",
       "         [-1.7795, -0.1171,  0.4707,  1.1404, -0.4600, -0.3640, -1.0923],\n",
       "         [-1.8796,  0.2238, -0.0867,  0.4162,  0.3026,  0.5182, -1.9219]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.last_hidden_state[:, :6, :7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
